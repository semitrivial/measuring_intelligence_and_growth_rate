\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\title{Transfinite extensions of Hibbard's intelligence measure}
\author{Samuel Alexander\thanks{The U.S.\ Securities and Exchange Commission}}
\date{2020}

\begin{document}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In his insightful paper \cite{hibbard}, Bill Hibbard introduces a novel
intelligence measure (which we will here refer to as the \emph{classical Hibbard measure})
for agents with Artificial General Intelligence (or AGIs).
Hibbard's measure is based on ``the game of adversarial sequence prediction
against a hierarchy of increasingly difficult sets of'' evaders (environments that attempt
to emit $1$s and $0$s in such a way as to evade prediction).
The levels of Hibbard's hierarchy are labelled by natural numbers\footnote{Technically,
Hibbard's hierarchy begins at level $1$ and he separately defines what it means for
an agent to have intelligence $0$, but that definition is equivalent to what would result
by declaring that the $0$th level of the hierarchy consists of the empty set of evaders.}, and
an agent's classical Hibbard measure is the maximum $n\in\mathbb N$ such that
said agent can eventually predict all the evaders in the $n$th level of the hierarchy,
or implicitly\footnote{Hibbard does not explicitly include the $\infty$ case in his
definition, but in his Proposition 3 he refers to agents having ``finite intelligence'', and
it is clear from context that by this he means agents who fail to predict some evader
somewhere in the hierarchy.} an agent's classical Hibbard measure is defined to be $\infty$
if said agent can eventually predict all the evaders in all levels of the hierarchy.

In this paper, we will argue that the evader-hierarchy which Hibbard originally proposed is
too small: we will argue that \emph{all} suitably idealized AGIs have $\infty$
intelligence according to the classical Hibbard measure. In order to remedy this
situation, we will propose a family of large evader-hierarchies. Each
evader-hierarchy in this family will admit a corresponding intelligence measure.
Whereas the classical Hibbard
measure is natural-number-valued, the intelligence measures obained from these larger
evader-hierarchies will be computable-ordinal-number-valued. Larger evader-hierarchies
will admit better intelligence measures, in the sense that the larger the evader-hierarchy,
the more difficult it will be for an AGI to have intelligence $\infty$ (i.e., for an
AGI to eventually predict all the evaders in all levels of said evader-hierarchy).

Our enlarged evader-hierarchies will be constructed using a technique
closely related to the so-called
\emph{majorization hierarchies} from mathematical logic \cite{weiermann2002slow}.
However, the majorization hierarchies in mathematical logic depend on so-called
\emph{fundamental sequences} which (in the long run) have problems both in
theory and practice, and so we modify the technique to be more concrete. Our modified
technique depends on what we call \emph{Intuitive Ordinal Notations}, which are more
concrete and practical (in the long run).

Finally, we will argue that, when applied to AGIs satisfying certain assumptions,
these extended Hibbard intelligence measures
are closely related to the so-called \emph{Intuitive Ordinal Intelligence} measure
which we introduced in \cite{ioi1} and \cite{ioi2}.

The structure of the paper is as follows.
\begin{itemize}
    \item
    In Section \ref{originalmeasuresection}, we review the classical Hibbard measure.
    \item
    In Section ..., we present our viewpoint of AGIs and argue that
    under certain idealizing assumptions, \emph{every} AGI should have intelligence
    $\infty$ according to the classical Hibbard measure.
    \item
    In Section ..., we review Intuitive Ordinal Notations.
    \item
    In Section ..., we describe a technique for constructing fast-growing functions
    $f:\mathbb N\to\mathbb N$ from Intuitive Ordinal Notations. This technique is
    closely related to the \emph{majorization hierarchies} from mathematical logic.
    \item
    In Section ..., we introduce a family of large evader-hierarchies and corresponding
    Hibbard intelligence measures.
    \item
    In Section ..., we discuss relationships between Hibbard intelligence measures
    and Intuitive Ordinal Intelligence.
\end{itemize}

\section{Hibbard's original measure}
\label{originalmeasuresection}

Hibbard's intelligence measure is based on predictors and evaders---the predictor
representing the AGI whose intelligence we would like to measure, and the evader
representing an environment---which we define below. A predictor and an evader
are thought of as interacting together in a ``game of adversarial sequence prediction''.

\begin{definition}
By $B$, we mean the binary alphabet $\{0,1\}$. By $B^*$, we mean the set of all
finite binary sequences. By $B^\infty$, we mean the set of all infinite binary
sequences. By $\langle\rangle$ we mean the empty binary sequence.
\end{definition}

\begin{definition}
\label{evaderpredictordefn}
    (Evaders and predictors)
    \begin{enumerate}
        \item
        By an \emph{evader}, we mean a Turing machine $e$
        which takes as input a finite (possibly empty) binary sequence
        $(y_1,\ldots,y_n)\in B^*$
        (thought of as a sequence of \emph{predictions})
        and outputs $0$ or $1$ (thought of as an \emph{evasion}), which output
        we write as $e(y_1,\ldots,y_n)$.
        \item
        By a \emph{predictor}, we mean a function $p:B^*\to B$
        which takes as input a finite (possibly empty) binary sequence
        $(x_1,\ldots,x_n)\in B^*$
        (thought of as a sequence of \emph{evasions})
        and outputs $0$ or $1$ (thought of as a \emph{prediction}).
        \item
        For any evader $e$ and predictor $p$, the \emph{result of $p$ playing the
        game of adversarial prediction against $e$} (or more simply, the \emph{result of
        $p$ playing against $e$}) is the infinite binary sequence
        $(x_1,y_1,x_2,y_2,\ldots)\in B^\infty$
        defined as follows:
        \begin{enumerate}
            \item
            $x_1=e(\langle\rangle)$ is
            the output of $e$ when run on the empty prediction-sequence.
            This is thought of as $e$'s first evasion.
            \item
            $y_1=p(\langle\rangle)$ is
            the result of applying $p$ to the empty evasion-sequence.
            This is thought of as $p$'s first prediction.
            \item
            For all $n>0$,
            $x_{n+1}=e(y_1,\ldots,y_n)$ is
            the output of $e$ on the sequence of $p$'s first $n$ predictions.
            This is thought of as $e$'s $(n+1)$th evasion.
            \item
            For all $n>0$,
            $y_{n+1}=p(x_1,\ldots,x_n)$ is
            the result of applying $p$ to $e$'s first $n$ evasions.
            This is thought of as $p$'s $(n+1)$th prediction.
        \end{enumerate}
        \item
        Suppose $r=(x_1,y_1,x_2,y_2,\ldots)$ is the result of a predictor $p$ playing
        against an evader $e$. For every $n\geq 1$,
        we say that \emph{the predictor wins round $n$ in $r$}
        if $x_n=y_n$; otherwise, if $x_n\neq y_n$, we say that
        \emph{the evader wins round $n$ in $r$}.
        We say that \emph{$p$ learns to predict $e$} if there is some $N\in\mathbb N$
        such that for all $n>N$, $p$ is the winner of round $n$ in $r$.
    \end{enumerate}
\end{definition}

\begin{definition}
    For any evader $e$ and natural $n>0$,
    let
    \[
        t_e(n) = \max_{b_1,\ldots,b_n\in \{0,1\}}
        (\text{number of steps $e$ takes to run on input $(b_1,\ldots,b_n)$}).
    \]
\end{definition}

\begin{example}
Let $e$ be an evader.
\begin{enumerate}
    \item
    $t_e(0)$ is simply the number of steps $e$ takes to run on input $\langle\rangle$,
    the empty binary sequence.
    \item
    $t_e(2)$ is equal to the number of steps $e$ takes to run on input
    $(0,0)$, or to run on input $(0,1)$, or to run on input $(1,0)$, or to run on input
    $(1,1)$---whichever of these four possibilities is largest.
\end{enumerate}
\end{example}

\begin{definition}
    Suppose $f:\mathbb N\to\mathbb N$. We define the \emph{evader-set bounded by $f$},
    written $E_f$, to be the set of all evaders $e$ such that
    there is some $k\in\mathbb N$ such that $\forall n>k$,
    $t_e(n)<f(n)$.
\end{definition}

\begin{definition}
    Suppose $g_i$ ($i\in \mathbb N$) is any family of functions,
    each $g_i:\mathbb N\to\mathbb N$.
    For each $m\in\mathbb N$, define $f_m:\mathbb N\to\mathbb N$ by
    \[f_m(k)=\max_{i\leq m}\max_{j\leq k}g_i(j).\]
    For any predictor $p$, the \emph{Hibbard intelligence of $p$ (given by the family $g_i$)}
    is defined to be the maximum $m$ such that
    $p$ learns to predict $e$ for every $e$ in $E_{f_m}$ (or $\infty$
    if $p$ learns to predict $e$ for every $e$ in $E_{f_m}$ for every $m$).
\end{definition}

\begin{definition}
    The \emph{classic Hibbard measure} is defined to be the Hibbard intelligence
    measure given by one specific family $g_i$ of functions, namely:
    the enumeration of the primitive recursive functions described by S.C.\ Liu
    \cite{liu1960enumeration}.
\end{definition}


\bibliographystyle{plain}
\bibliography{hibbard}
\end{document}
