\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}

\title{Transfinite extensions of Hibbard's intelligence measure}
\author{Samuel Alexander\thanks{The U.S.\ Securities and Exchange Commission}}
\date{2020}

\begin{document}

\maketitle

\begin{abstract}
    Todo:
    \begin{enumerate}
        \item Rewrite 'caution'.
        \item Fix the subscripts in generalintelligencemeasuredefn to start at 0.
    \end{enumerate}
\end{abstract}

\section{Introduction}

In his insightful paper \cite{hibbard}, Bill Hibbard introduces a novel
intelligence measure (which we will here refer to as the \emph{classical Hibbard measure})
for agents with Artificial General Intelligence (or AGIs).
Hibbard's measure is based on ``the game of adversarial sequence prediction
against a hierarchy of increasingly difficult sets of'' evaders (environments that attempt
to emit $1$s and $0$s in such a way as to evade prediction).
The levels of Hibbard's hierarchy are labelled by natural numbers\footnote{Technically,
Hibbard's hierarchy begins at level $1$ and he separately defines what it means for
an agent to have intelligence $0$, but that definition is equivalent to what would result
by declaring that the $0$th level of the hierarchy consists of the empty set of evaders.}, and
an agent's classical Hibbard measure is the maximum $n\in\mathbb N$ such that
said agent can eventually predict all the evaders in the $n$th level of the hierarchy,
or implicitly\footnote{Hibbard does not explicitly include the $\infty$ case in his
definition, but in his Proposition 3 he refers to agents having ``finite intelligence'', and
it is clear from context that by this he means agents who fail to predict some evader
somewhere in the hierarchy.} an agent's classical Hibbard measure is defined to be $\infty$
if said agent can eventually predict all the evaders in all levels of the hierarchy.

In this paper, we will argue that the evader-hierarchy which Hibbard originally proposed is
trivial: we will argue that \emph{all} suitably idealized AGIs probably have $\infty$
intelligence according to the classical Hibbard measure.
We will suggest two different methods to repair Hibbard's intelligence measure.
\begin{itemize}
    \item
    The first method we describe will be elementary, in the sense that it will
    essentially involve
    no mathematics more advanced than the mathematics used in Hibbard's original approach.
    However, we will show that the intelligence measure yielded by this elementary
    method has a crucial flaw.
    \item
    The second method we describe will make use of two advanced notions from
    mathematical logic: the \emph{slow-growing hierarchy}
    \cite{weiermann2002slow} and the ordinal notation system known as
    \emph{Kleene's $\mathcal O$} \cite{kleene1938notation}.
\end{itemize}
Both of these methods will, rather than producing a single intelligence measure,
rather, produce a family of intelligence measures indexed by a parameter $X$ where
$X$ is an AGI. In other words, for every AGI $X$, for each of the above two methods,
there will be a corresponding intelligence measure, which can informally be thought
of as ``intelligence as judged by $X$'': $X$ itself will have intelligence $\infty$
according to the intelligence measure corresponding to $X$, but some other AGI $Y$
may have non-$\infty$ intelligence. This captures the intuition that an entity who
measures intelligence of less intelligent entities cannot accurately measure
its own intelligence, nor the intelligence of entities more intelligent than itself.

The structure of the paper is as follows.
\begin{itemize}
    \item
    In Section \ref{originalmeasuresection}, we review the classical Hibbard measure.
    \item
    In Section \ref{agiperspectivesection}, we present our viewpoint of AGIs and argue that
    under certain idealizing assumptions, \emph{every} AGI probably has intelligence
    $\infty$ according to the classical Hibbard measure.
    \item
    In Section \ref{simplemeasuresection}, we present an elementary way to
    remedy Hibbard's intelligence
    measure so that not every AGI has intelligence $\infty$; however, the resulting
    intelligence measures are shown to still have a crucial flaw.
    \item
    In Section ..., we review the \emph{slow-growing hierarchy}.
    \item
    In Section ..., we review \emph{Kleene's $\mathcal O$}.
    \item
    In Section ..., we use the slow-growing hierarchy and Kleene's $\mathcal O$
    to present another way to
    remedy Hibbard's intelligence measure. This resulting intelligence measures do not
    have the flaw that is seen in Section ....
\end{itemize}

\section{Hibbard's original measure}
\label{originalmeasuresection}

Hibbard's intelligence measure is based on predictors and evaders---the predictor
representing the AGI whose intelligence we would like to measure, and the evader
representing an environment---which we define below. A predictor and an evader
are thought of as interacting together in a ``game of adversarial sequence prediction''.

\begin{definition}
By $B$, we mean the binary alphabet $\{0,1\}$. By $B^*$, we mean the set of all
finite binary sequences. By $B^\infty$, we mean the set of all infinite binary
sequences. By $\langle\rangle$ we mean the empty binary sequence.
\end{definition}

\begin{definition}
\label{evaderpredictordefn}
    (Evaders and predictors)
    \begin{enumerate}
        \item
        By an \emph{evader}, we mean a Turing machine $e$
        which takes as input a finite (possibly empty) binary sequence
        $(y_1,\ldots,y_n)\in B^*$
        (thought of as a sequence of \emph{predictions})
        and outputs $0$ or $1$ (thought of as an \emph{evasion}), which output
        we write as $e(y_1,\ldots,y_n)$.
        \item
        By a \emph{predictor}, we mean a function $p:B^*\to B$
        which takes as input a finite (possibly empty) binary sequence
        $(x_1,\ldots,x_n)\in B^*$
        (thought of as a sequence of \emph{evasions})
        and outputs $0$ or $1$ (thought of as a \emph{prediction}).
        \item
        For any evader $e$ and predictor $p$, the \emph{result of $p$ playing the
        game of adversarial prediction against $e$} (or more simply, the \emph{result of
        $p$ playing against $e$}) is the infinite binary sequence
        $(x_1,y_1,x_2,y_2,\ldots)\in B^\infty$
        defined as follows:
        \begin{enumerate}
            \item
            $x_1=e(\langle\rangle)$ is
            the output of $e$ when run on the empty prediction-sequence.
            This is thought of as $e$'s first evasion.
            \item
            $y_1=p(\langle\rangle)$ is
            the result of applying $p$ to the empty evasion-sequence.
            This is thought of as $p$'s first prediction.
            \item
            For all $n>0$,
            $x_{n+1}=e(y_1,\ldots,y_n)$ is
            the output of $e$ on the sequence of $p$'s first $n$ predictions.
            This is thought of as $e$'s $(n+1)$th evasion.
            \item
            For all $n>0$,
            $y_{n+1}=p(x_1,\ldots,x_n)$ is
            the result of applying $p$ to $e$'s first $n$ evasions.
            This is thought of as $p$'s $(n+1)$th prediction.
        \end{enumerate}
        \item
        Suppose $r=(x_1,y_1,x_2,y_2,\ldots)$ is the result of a predictor $p$ playing
        against an evader $e$. For every $n\geq 1$,
        we say that \emph{the predictor wins round $n$ in $r$}
        if $x_n=y_n$; otherwise, if $x_n\neq y_n$, we say that
        \emph{the evader wins round $n$ in $r$}.
        We say that \emph{$p$ learns to predict $e$} if there is some $N\in\mathbb N$
        such that for all $n>N$, $p$ is the winner of round $n$ in $r$.
    \end{enumerate}
\end{definition}

\begin{definition}
\label{tsubedefinition}
    For any evader $e$ and natural $n>0$,
    let
    \[
        t_e(n) = \max_{b_1,\ldots,b_n\in \{0,1\}}
        (\text{number of steps $e$ takes to run on input $(b_1,\ldots,b_n)$}).
    \]
\end{definition}

\begin{example}
Let $e$ be an evader.
\begin{enumerate}
    \item
    $t_e(0)$ is simply the number of steps $e$ takes to run on input $\langle\rangle$,
    the empty binary sequence.
    \item
    $t_e(2)$ is equal to the number of steps $e$ takes to run on input
    $(0,0)$, or to run on input $(0,1)$, or to run on input $(1,0)$, or to run on input
    $(1,1)$---whichever of these four possibilities is largest.
\end{enumerate}
\end{example}

\begin{definition}
\label{evadersetdefinition}
    Suppose $f:\mathbb N\to\mathbb N$. We define the \emph{evader-set bounded by $f$},
    written $E_f$, to be the set of all evaders $e$ such that
    there is some $k\in\mathbb N$ such that $\forall n>k$,
    $t_e(n)<f(n)$.
\end{definition}

\begin{definition}
\label{generalintelligencemeasuredefn}
    Suppose $g_i$ ($i\in \mathbb N$) is any family of functions,
    each $g_i:\mathbb N\to\mathbb N$.
    For each $m\in\mathbb N$, define $f_m:\mathbb N\to\mathbb N$ by
    \[f_m(k)=\max_{i\leq m}\max_{j\leq k}g_i(j).\]
    For any predictor $p$, the \emph{Hibbard intelligence of $p$ (given by the family $g_i$)}
    is defined to be the maximum $m$ such that
    $p$ learns to predict $e$ for every $e$ in $E_{f_m}$ (or $\infty$
    if $p$ learns to predict $e$ for every $e$ in $E_{f_m}$ for every $m$).
\end{definition}

\begin{definition}
\label{classichibbardmeasuredefn}
    The \emph{classic Hibbard measure} is defined to be the Hibbard intelligence
    measure given by one specific family $g_i$ of functions, namely:
    the enumeration of the primitive recursive functions described by S.C.\ Liu
    \cite{liu1960enumeration}.
\end{definition}


\section{Do all AGIs have classic Hibbard intelligence $\infty$?}
\label{agiperspectivesection}

As yet, we are unaware of any formal mathematical definition of what an AGI
actually is. We take the stance that an AGI should be capable of understanding
practical conversations and instructions in a common human language (such as
English) and, if commanded by their employers, to obey commands expressed
therein. So if the AGI's employer said, ``Recite digits of pi until I tell you to
stop,'' then the AGI would begin computing and reciting digits of pi, until said
employer told it to stop (if ever). Unlike a human employee, who would eventually
get fed up with such a task, an AGI should have no fear of tedium, and should
never grow tired of performing whatever task it has been assigned.

We think assumptions like the above are implicit in many attempts to study AGIs.
Many authors who set out to study
AGIs actually study not the full AGI, but rather the AGI's persona when it is
performing some general type of task. For example:
\begin{itemize}
    \item
    Legg and Hutter measure the intelligence of AGIs based on their performance
    in reinforcement learning environments \cite{legg}. On a superficial
    level, that approach is inappropriate because a genuine AGI is much more than
    merely a reinforcement learning agent (however good that agent might be).
    But at a deeper level, the approach is appropriate because although an AGI is
    more than an RL agent, we could command the AGI to act as an RL agent, saying,
    ``I am going to put you in a certain environment where you can take such-and-such
    tasks, and rewards and observations will be communicated to you; please try to
    reverse-engineer the environment and act in such a way as to maximize rewards.''
    And while it is difficult to say anything about a full AGI, we can say quite a
    lot about an RL agent, and thus we can study AGIs by proxy.
    \item
    In the Non-Axiomatic Reasoning System research program (NARS) \cite{nars},
    AGIs are reduced to agents who are bombarded with assertions in a specific formal
    language, each assertion being accompanied by a priority measure, and some of
    which assertions might contradict each-other. At any time during the bombardment
    of assertions, the agent might also be queried, at which point the agent's job is
    to estimate the probability of the truth of a queried assertion, based on the
    assertions and priorities it has seen so far. This is certainly not all an AGI
    is (for an AGI should be able to reason about first- and higher-order logics
    involving quantifiers, for example). Nevertheless, an AGI's employer could command
    it to behave like such an agent, and in response the AGI would behave like such an
    agent.
    \item
    Many philosophers and logicians reduce an AGI to a mere \emph{knowing
    agent} who performs no actions but simply knows things in some formal language.
    Superficially, this is inappropriate because an AGI does much more than merely
    know things \cite{wang2007three}. But at a deeper level, this is appropriate
    because an AGI could certainly be commanded to act as such an agent, e.g., via 
    a command like: ``Until further notice, do nothing except enumerate consequences
    of Peano arithmetic'' (or some other formal language). And again, while it is
    hard to say anything about a full AGI, we can say a lot about such computably
    enumerable theories as would be recited by an AGI in response to such a question
    (and they would indeed be computably enumerable because an AGI, whatever else it
    is, must be mechanical).
\end{itemize}

In the same way, an AGI is certainly more than a predictor competing in
games of adversarial sequence prediction. But an AGI could be commanded to
act as such a predictor. That is why it makes sense to refer to the measures
from Definition \ref{generalintelligencemeasuredefn} as ``intelligence'' measures
(rather than as, say, ``prediction power'' measures). In the same way that
IQ tests are intended to measure overall human intelligence despite the fact that
human intelligence is so much more than the ability to take IQ tests, in the same
way, adversarial sequence prediction games can serve as a sort of intelligence
test for AGIs.

In the remainder of this section, we will argue that under certain idealizing
assumptions, every AGI should have classical Hibbard intelligence $\infty$.
Thus, we would suggest that Definition \ref{classichibbardmeasuredefn} is really
only appropriate for certain weak AIs who are strong enough that they can be
induced to participate in adversarial sequence prediction games, but who are
not strong enough to qualify as genuine AGIs. Later in the paper, we will argue
that suitable extensions of Hibbard's intelligence measure are more relevant
for genuine AGIs.

\begin{assumption}
\label{idealizingassumption}
    We make the following idealizing assumptions about an AGI $X$.
    \begin{itemize}
        \item
        (Mathematical truthfulness) If $X$ knows some statement in the language
        of Peano arithmetic, then that statement is true.
        \item
        (Mathematical reasoning ability) If $X$ knows some set $S$ of mathematical
        axioms then, given sufficient time to think about it, $X$ would eventually
        become aware of all the consequences of $S$.
        \item
        (Basic mathematical exposure) $X$ knows the axioms of Peano arithmetic.
        \item
        (Caution) $X$ avoids infinite loops: if $X$ is commanded to play the
        role of a predictor, then the resulting predictor $p$ will indeed
        be a predictor (a total computable function
        from $B^*$ to $B$).
    \end{itemize}
\end{assumption}

\begin{definition}
\label{Xspredictordefn}
    Suppose $X$ is an AGI.
    \begin{enumerate}
        \item
        \emph{$X$'s predictor} is the predictor which
        would result if $X$ were commanded to play the role of a predictor (so
        the $X$'s predictor is a total computable function from $B^*\to B$ by Caution).
        \item
        For any family $g_i$ of functions as in
        Definition \ref{generalintelligencemeasuredefn},
        the \emph{Hibbard intelligence of $X$ (given by the family $g_i$)}
        is defined to be the Hibbard intelligence of
        $X$'s predictor (given by the family $g_i$).
    \end{enumerate}
\end{definition}

\subsection{A semi-optimal predictor}

\begin{definition}
\label{bruteforcepredictordefn}
    (The Brute-Force Predictor)
    Suppose $X$ is an AGI. Let $M_1,M_2,\ldots$ be the list of Turing machines
    which $X$ would enumerate if $X$ were commanded: ``Enumerate all the Turing
    machines which you know define total functions from $B^*$ to $B$''.
    By \emph{$X$'s brute-force predictor}, we mean the predictor $p$ defined
    as follows.
    \begin{enumerate}
        \item
        Initially, $p$ shall attempt to predict the evader $e$ by assuming that $e$
        defines the same function as $M_1$. When (if ever) this fails,
        say that \emph{$M_1$ is ruled out from being the evader}.
        \item
        Once $M_1$ has been ruled out from being the evader (if ever),
        $p$ shall attempt to predict $e$ by assuming that $e$ defines the same
        function as $M_2$. When (if ever) this fails,
        say that \emph{$M_2$ is ruled out from being the evader}.
        \item
        Once $M_2$ has been ruled out from being the evader (if ever),
        $p$ shall attempt to predict $e$ by assuming that $e$ defines the same
        function as $M_3$. When (if ever) this fails,
        say that \emph{$M_3$ is ruled out from being the evader}.
        \item
        And so on forever...
    \end{enumerate}
\end{definition}

Note that $X$'s predictor (Definition \ref{Xspredictordefn})
may or may not be equal to $X$'s brute-force predictor
(Definition \ref{bruteforcepredictordefn}). Note also that $X$'s brute-force predictor
is total precisely because of the mathematical truthfulness assumption
(from Definition \ref{idealizingassumption}): whenever $X$ knows that
$M_i$ defines a total function from $B^*$ to $B$, $M_i$ really \emph{does}
define a total function from $B^*$ to $B$ (hereafter, we will suppress
remarks like this and routinely use the mathematical truthfulness assumption
without explicit mention).

\begin{lemma}
\label{knowingimplieslearninglemma}
    Let $X$ be an AGI and let $e$ be an evader (so $e$ is a Turing machine which
    computes a function from $B^*$ to $B$).
    Let $M$ be any Turing machine which computes the same function from $B^*$ to $B$
    as $e$ computes.
    If $X$ knows that $M$ defines a total function from $B^*$ to $B$,
    then $X$'s brute-force predictor learns to predict $e$.
\end{lemma}

\begin{proof}
    Let $p$ be $X$'s brute-force predictor.
    Let $M_1,M_2,\ldots$ be as in Definition \ref{bruteforcepredictordefn}.
    Since $X$ knows that $M$ defines a total
    function from $B^*$ to $B$,
    it follows that $M=M_k$ for some $k$.
    When $p$ plays against $e$, it cannot occur that $M_k$ is ruled out
    from being the evader, because in order for that to occur, $p$ would have
    to fail at predicting $e$ when $p$ assumes that $e$ computes the same
    function as $M_k$, but that cannot fail because that assumption is true.
    Since $M_k$ cannot be ruled out from being the evader, it follows that
    step $k+1$ of Definition \ref{bruteforcepredictordefn} will never be
    reached, which in turn implies that $p$ stops failing at predicting $e$
    after finitely many initial failures, in other words, $p$ learns to
    predict $e$.
\end{proof}

For any AGI $X$, we would like to claim that $X$'s brute-force predictor is
optimal among all predictors which $X$ could make use of without violating the
Caution assumption (Definition \ref{idealizingassumption}); unfortunately this
is not true in the strongest sense it possibly could be true, as the following
example shows.

\begin{example}
\label{bruteforcenottotallyoptimalexample}
    Let $X$ be an AGI. Let $f:\mathbb N\to B$ be a total computable function which
    is different from every total computable function $X$ knows.
    For each $i\in\{0,1\}$, $e_i$ be an evader defined as follows:
    \[
        e_i(y_1,\ldots,y_n) =
        \begin{cases}
            i &\mbox{if $y_1=\cdots=y_n=i$ (or if $n=0$),}\\
            f(n) &\mbox{otherwise.}
        \end{cases}
    \]
    For each $i\in\{0,1\}$, let $p_i$ be the
    constantly-$i$ predictor, $p_i(x_1,\ldots,x_n)=i$.
    Then $p_0$ learns to predict $e_0$ and $p_1$ learns to predict $e_1$,
    but $X$'s brute-force predictor learns to predict at most one of $e_0$ or $e_1$.
\end{example}

\begin{proof}
    Clearly the result of $p_0$ playing against $e_0$ is
    $(x_0,y_0,x_1,y_1,\ldots)=(0,0,0,0,\ldots)$, so $p_0$ learns to predict $e_0$.
    Likewise, the result of $p_1$ playing against $e_1$ is
    $(x_0,y_0,x_1,y_1,\ldots)=(1,1,1,1,\ldots)$, so $p_1$ learns to predict $e_1$.
    It remains to show that $X$'s brute-force predictor cannot learn to predict
    both $e_0$ and $e_1$.

    Let $p$ be $X$'s brute-force predictor.
    Let $M_1,M_2,\ldots$ be as in Definition \ref{bruteforcepredictordefn}.
    Let $g:B^*\to B$ be the function defined by $M_1$.

    Case 1: $g(\langle\rangle)=1$. Then when $p$ plays against $e_0$,
    after the first step, it will never be the case that
    $y_1=\cdots=y_n=0$ (because $y_1=g(\langle\rangle)=1$).
    Thus, for all $n>0$, $e_0(y_1,\ldots,y_n)=f(n)$.
    I claim that $p$ does not learn $e_0$.
    To see this, assume (for the sake of contradiction)
    that $p$ learns $e_0$. It follows that there is some $k$ (which we may take
    as small as possible) such that $M_k$ never gets ruled out from being the
    evader (Definition \ref{bruteforcepredictordefn}). Let $h:B^*\to B$ be the
    function defined by $M_k$.
    Let $(x_0,y_0,x_1,y_1,\ldots)$ be the result of $p$ playing against $e_0$.
    It follows that there is some $j$ such that for all $i>j$,
    $h(y_1,\ldots,y_{i-1})=x_i=y_i=f(i)$.
    This shows that for all but finitely many $i\in\mathbb N$,
    $f(i)=h(y_1,\ldots,y_{i-1})$. Since $X$ knows that $M_k$ defines a total
    function from $B^*$ to $B$, it follows that $X$ knows $f$ is total computable,
    which contradicts our choice of $f$.

    Case 2: $g(\langle\rangle)=0$. Then by similar reasoning as in Case 1,
    it can be shown that $p$ does not learn $e_1$.
\end{proof}

Example \ref{bruteforcenottotallyoptimalexample} shows that we cannot
hope for the brute-force to be totally optimal in the most extreme possible sense:
there will always be evaders that the AGI's brute-force fails to learn, but which other
predictors (which the AGI knows about)
do nevertheless learn. Example \ref{bruteforcenottotallyoptimalexample} involves
highly contrived evaders which are custom made to act stupidly in one specific case
(allowing them to be learned by a correspondingly stupid predictor), while being
highly sophisticated in all other cases. In the following definition, we rule out
situations where a less sophisticated predictor manages to learn a
more sophisticated evader due to the latter cloaking its true sophistication from
the former.

\begin{definition}
    Suppose $M$ is a Turing machine which computes a predictor $p$.
    Suppose $e$ is an evader.
    We say that \emph{$M$ learns $e$ but-not-without-a-fight}
    if the following conditions hold:
    \begin{enumerate}
        \item $p$ learns $e$.
        \item For each $i$, $t_M(i)\geq t_e(i)$.
    \end{enumerate}
\end{definition}

\begin{theorem}
\label{semioptimalitytheorem}
    (Semi-optimality of brute force)
    Let $X$ be an AGI.
    Suppose $M$ is a Turing machine which computes a predictor $p$.
    If $X$ knows that $M$ computes a predictor, and if $M$ learns $e$
    but-not-without-a-fight, then $X$'s brute-force predictor learns $e$.
\end{theorem}

\begin{proof}
    Since $X$ knows $M$ computes a predictor, $X$ knows $M$ computes a total
    function from $B^*$ to $B$. Thus $X$ knows that that $e'$ is a total
    computable function from $B^*$ to $B$, where $e'$ is a Turing machine which
    takes an input $b\in B^*$ and operates as follows:
    \begin{enumerate}
        \item
        Calculate $t_M(i)$ (by running $M$ on all length-$i$ binary sequences).
        \item
        Run $e$ on $b$ up to $t_M(i)$ steps. If $e$ outputs a result $x$ within that
        time, then output $x$. Otherwise, output $0$.
    \end{enumerate}
    Since $M$ learns $e$ but-not-without-a-fight, each $t_M(i)\geq t_e(i)$,
    so in fact $e'$ computes the same function as $e$.
    By Lemma \ref{knowingimplieslearninglemma}, $X$'s brute-force predictor
    learns $e'$. Since $e'$ and $e$ compute the same function, this implies
    $X$'s brute-force predictor learns $e$.
\end{proof}

\subsection{Triviality of the classical Hibbard measure}
\label{trivialitysubsection}

Although Example \ref{bruteforcenottotallyoptimalexample} showed that
an AGI $X$'s brute-force predictor
is not optimal in the strongest possible sense, Theorem \ref{semioptimalitytheorem}
shows that $X$'s brute-force predictor is still semi-optimal. In some sense,
$X$'s brute-force predictor learns every evader which any other predictor $p$ (that $X$
knows is a predictor) would learn, except possibly for cases where $p$ only learns
an evader $e$ because $e$ withholds its full sophistication from $p$.

Based on the above, we informally conjecture that the brute-force predictor is
the most sensible predictor for an AGI $X$ to act as, if $X$ is commanded to
act as a predictor and try to predict as many evaders as possible. If this conjecture
is true, then the following theorem suggests that the classical Hibbard measure is
trivial: that it assigns intelligence level $\infty$ to every AGI.

\begin{theorem}
    Suppose $X$ is an AGI. If $X$'s predictor is $X$'s brute-force predictor,
    then $X$ has classical Hibbard intelligence $\infty$.
\end{theorem}

\begin{proof}
    By Definition \ref{Xspredictordefn}, $X$'s classical Hibbard intelligence
    equals the classical Hibbard intelligence of $X$'s predictor, which is
    $X$'s brute-force predictor by assumption. Let $p$ be $X$'s brute-force
    predictor. By Definition \ref{generalintelligencemeasuredefn}, in order
    to show $p$ has classical Hibbard intelligence $\infty$, we must
    show that $p$ learns to predict $e$ for every $e$ in $E_{f_m}$ for
    every $m\in\mathbb N$,
    where each $f_m:\mathbb N\to\mathbb N$ is defined by
    $f_m(k)=\max_{i\leq m}\max_{j\leq k}g_i(j)$,
    where $g_i$ is the $i$th primitive recursive function
    (Definition \ref{classichibbardmeasuredefn}).

    Let $m\in\mathbb N$ be arbitrary and let $e\in E_{f_m}$ be arbitrary.
    By Definition \ref{evadersetdefinition},
    for all but finitely many $n\in\mathbb N$, $t_e(n)<f_m(n)$.
    We must show that $p$ learns to predict $e$.

    Let $M$ be a Turing machine which takes an input
    $(y_1,\ldots,y_n)\in B^*$ and
    operates as follows:
    \begin{itemize}
        \item
        Spend up to $f_m(n)$ steps computing $e(y_1,\ldots,y_n)$.
        If $e$ halts during that time, say with output $x$, then output $x$.
        Otherwise, output $0$.
    \end{itemize}
    Since, for all but finitely many $n\in\mathbb N$, $t_e(n)<f_m(n)$,
    it follows that $M$ computes the same function as $e$ except for finitely
    many exceptions.

    It is well-known that for any primitive recursive function $g$, Peano
    arithmetic proves that $g$ is total. Thus, by the \emph{basic mathematical
    exposure} and \emph{mathematical reasoning ability} assumptions from
    Assumption \ref{idealizingassumption}, it follows that $X$ knows that
    $M$ is total. Since $M$ computes the same function as $e$ except for
    finitely many exceptions, it follows that there is a Turing machine $M'$
    which computes the same function as $e$ all thet time, and such that $X$
    knows that $M'$ is total. By Lemma \ref{knowingimplieslearninglemma},
    $p$ learns to predict $e$, as desired.
\end{proof}

\section{An elementary method to non-trivialize Hibbard's intelligence measure}
\label{simplemeasuresection}

In Subsection \ref{trivialitysubsection}, we saw that every AGI probably has
intelligence $\infty$ according to the classical Hibbard measure. If so, this
means the classical Hibbard measure is trivial. In this section, we will present
the first of two methods for remedying this state of affairs. Rather than define
one single intelligence measure, we will define a family of intelligence measures:
one intelligence measure for each AGI $X$, which one might think of informally as
a measure of ``intelligence as judged by $X$''.

The idea is simple. The classical Hibbard measure
(Definition \ref{classichibbardmeasuredefn}) depends on a sequence $(g_0,g_1,\ldots)$
of total computable functions $g_i:\mathbb N\to\mathbb N$ imported from one specific
outside source, namely Liu \cite{liu1960enumeration}. Rather than artificially depend
on Liu, we will instead depend on an AGI $X$.

\begin{definition}
\label{functionlistdefinition}
    If $X$ is an AGI, let $(C^X_1, C^X_2, \ldots)$ be the sequence of
    Turing machines which would result if $X$ were commanded: ``Until further
    notice, list all the Turing machines $C$ that you can think of such that
    you know $C$ computes a total function from $\mathbb N$ to $\mathbb N$.''
    For each $i>0$, let $g^X_i:\mathbb N\to\mathbb N$ be the function computed by
    $C^X_i$.
\end{definition}

For example, if $X$ is an AGI, then in response to the command in Definition
\ref{functionlistdefinition}, $X$ might think for a while, figure out a Turing
machine $C^X_1$ which computes the constant function $g^X_1(n)=0$. Then $X$
might think some more and figure out a Turing machine $C^X_2$ which computes
the identity function $g^X_2(n)=n$. Then $X$ might think some more and figure
out a Turing machine $C^X_3$ which computes the prime number function, so that
$g^X_3(n)$ is the $(n+1)$th prime number. And so on.

It is philosophically interesting that although there is no apparent ordering that
would turn the set of all total-function-from-$\mathbb N$-to-$\mathbb N$-computing
Turing machines into a sequence, an AGI automatically acts as a kind of oracle for
so ordering said Turing machines (or at least, those which said AGI knows to be in
said set).

\begin{definition}
    For any AGI $X$, the \emph{simple Hibbard measure given by $X$}
    is defined to be the Hibbard measure given by
    $(g^X_1,g^X_2,\ldots$) (see Definition \ref{generalintelligencemeasuredefn}).
    For any AGI $Y$, we will write $|Y|_X$ for the intelligence of $Y$
    according to the simple Hibbard measure given by $X$.
\end{definition}

\subsection{Non-triviality of the simple Hibbard measures}

We will argue that, at least for certain AGIs $X$,
the simple Hibbard measure given by $X$ is
not trivial in the same way that the classical Hibbard measure is:
that at least for certain $X$, there exist $Y$ such that
$|Y|_X$ is finite.

\begin{theorem}
\label{simplehibbardnontrivialtheorem}
    (Non-triviality of simple Hibbard measures)
    Suppose $X$ and $Y$ are AGIs. Let $p$ be $Y$'s predictor.
    If $X$ knows that $p$ defines a total function from $B^*$ to $B$,
    then $|Y|_X<\infty$.
\end{theorem}

\begin{proof}
    Let $e$ be the evader which attempts to evade the predictor by assuming
    that the predictor is $p$ and always outputting the opposite of what
    the predictor will output based on that assumption. Clearly, $p$ does
    not learn to predict $e$.

    Since $X$ knows that $p$ defines a total function from $B^*$ to $B$,
    it follows that there is a Turing machine $C$ such that:
    \begin{enumerate}
        \item
        $C$ computes the function $g:\mathbb N\to \mathbb N$ defined by
        $g(n)=t_e(n)+1$, where $t_e$ is as in
        Definition \ref{tsubedefinition}.
        \item
        $X$ knows that $C$ computes a total function from $\mathbb N$ to $\mathbb N$.
    \end{enumerate}
    By (2), $C=C^X_m$ for some $m$, and thus $g^X_m=g$.
    So if $f_m:\mathbb N\to\mathbb N$ is defined
    (as in Definition \ref{generalintelligencemeasuredefn})
    as $f_m(k)=\max_{i\leq m}\max_{j\leq k}g^X_i(j)$,
    then for every $k$, $f_m(k)\geq g^X_m(k)=g(k)=t_e(k)+1>t_e(k)$.
    This shows that $e\in E_{f_m}$ (from Definition \ref{evadersetdefinition}).
    Since $p$ does not learn to predict $e$,
    it is not the case that $p$ learns to predict every evader in $E_{f_m}$.
    Therefore, $|Y|_X<m$.
\end{proof}

\subsection{The problem with simple Hibbard measures}

In Theorem \ref{simplehibbardnontrivialtheorem} we proved more than was necessary,
yielding the following stronger result.

\begin{corollary}
\label{technicalcorollaryaboutsimplehibbardmeasures}
    Suppose $X$ and $Y$ are AGIs. Let $p$ be $Y$'s predictor.
    Suppose that $X$ knows that $p$ defines a total function from
    $B^*$ to $B$.
    Then there is a Turing machine $C$ such that:
    \begin{enumerate}
        \item
        $C$ computes the function $g:\mathbb N\to \mathbb N$ defined by
        $g(n)=t_e(n)+1$, where $t_e$ is as in
        Definition \ref{tsubedefinition}.
        \item
        $X$ knows that $C$ computes a total function from $\mathbb N$ to $\mathbb N$.
    \end{enumerate}
    Furthermore: $|Y|_X<m$ where $m$ is such that $C=C^X_m$.
\end{corollary}

On the other hand, the following result also holds.

\begin{lemma}
\label{technicallemmaaboutsimplehibbardmeasures}
    Suppose $X$ and $Y$ are AGIs. Suppose that for all $i=1,\ldots,m$,
    $g^X_i$ is identically zero: $g^X_i(n)=0$.
    Then $|Y|_X\geq m$.
\end{lemma}

\begin{proof}
    If $f_m:\mathbb N\to\mathbb N$
    is defined (as in Definition \ref{generalintelligencemeasuredefn})
    by $f_m(k)=\max_{i\leq m}\max_{j\leq k}g^X_i(j)$, then, by
    assumption, $f_m(k)=\max_{i\leq m}\max_{j\leq k}0=0$.
    Thus $E_{f_m}$ is empty, and so, vacuously, $Y$'s predictor learns
    to predict every evader in $E_{f_m}$. This shows $|Y|_X\geq m$.
\end{proof}

By the \emph{mathematical reasoning ability} and \emph{basic mathematical
exposure} assumptions from Assumption \ref{idealizingassumption},
for any AGI $X$, there are infinitely many Turing machines $C$ such that
\begin{enumerate}
    \item
    $C$ defines the zero function $g(n)=0$ defined on all of $\mathbb N$, and
    \item
    $X$ knows $C$ defines a total function from $\mathbb N$ to $\mathbb N$.
\end{enumerate}
Thus, combining Corollary \ref{technicalcorollaryaboutsimplehibbardmeasures}
and Lemma \ref{technicallemmaaboutsimplehibbardmeasures},
we see that if $X$ knows that $Y$'s predictor defines a total function
from $B^*$ to $B$, then we can make $|Y|_{X'}$ have any value we like,
by letting $X'$ be an AGI identical to $X$ in every way except that
$X'$ outputs Turing machines in a different order than $X$ when commanded
as in Definition \ref{functionlistdefinition}.
This is a crucial flaw in the simple Hibbard measures, because it shows
that $|Y|_X$ depends just as much on the arbitrary order in which $X$
outputs Turing machines as it does on how intelligent $Y$ is.

In Section ..., we will define a more sophisticated type of Hibbard measure
which avoids the above problem.

\bibliographystyle{plain}
\bibliography{hibbard}
\end{document}
