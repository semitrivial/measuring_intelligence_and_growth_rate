\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}

\title{Transfinite extensions of Hibbard's intelligence measure}
\author{Samuel Alexander\thanks{The U.S.\ Securities and Exchange Commission}}
\date{2020}

\begin{document}

\maketitle

\begin{abstract}
    Fill this in.
\end{abstract}

\section{Introduction}

In his insightful paper \cite{hibbard}, Bill Hibbard introduces a novel
intelligence measure (which we will here refer to as the \emph{classical Hibbard measure})
for agents with Artificial General Intelligence (or AGIs).
Hibbard's measure is based on ``the game of adversarial sequence prediction
against a hierarchy of increasingly difficult sets of'' evaders (environments that attempt
to emit $1$s and $0$s in such a way as to evade prediction).
The levels of Hibbard's hierarchy are labelled by natural numbers\footnote{Technically,
Hibbard's hierarchy begins at level $1$ and he separately defines what it means for
an agent to have intelligence $0$, but that definition is equivalent to what would result
by declaring that the $0$th level of the hierarchy consists of the empty set of evaders.}, and
an agent's classical Hibbard measure is the maximum $n\in\mathbb N$ such that
said agent can eventually predict all the evaders in the $n$th level of the hierarchy,
or implicitly\footnote{Hibbard does not explicitly include the $\infty$ case in his
definition, but in his Proposition 3 he refers to agents having ``finite intelligence'', and
it is clear from context that by this he means agents who fail to predict some evader
somewhere in the hierarchy.} an agent's classical Hibbard measure is defined to be $\infty$
if said agent can eventually predict all the evaders in all levels of the hierarchy.

In this paper, we will argue that the evader-hierarchy which Hibbard originally proposed is
too small: we will argue that \emph{all} suitably idealized AGIs have $\infty$
intelligence according to the classical Hibbard measure. In order to remedy this
situation, we will propose a family of large evader-hierarchies. Each
evader-hierarchy in this family will admit a corresponding intelligence measure.
Whereas the classical Hibbard
measure is natural-number-valued, the intelligence measures obained from these larger
evader-hierarchies will be computable-ordinal-number-valued. Larger evader-hierarchies
will admit better intelligence measures, in the sense that the larger the evader-hierarchy,
the more difficult it will be for an AGI to have intelligence $\infty$ (i.e., for an
AGI to eventually predict all the evaders in all levels of said evader-hierarchy).

Our enlarged evader-hierarchies will be constructed using a technique
closely related to the so-called
\emph{majorization hierarchies} from mathematical logic \cite{weiermann2002slow}.
However, the majorization hierarchies in mathematical logic depend on so-called
\emph{fundamental sequences} which (in the long run) have problems both in
theory and practice, and so we modify the technique to be more concrete. Our modified
technique depends on what we call \emph{Intuitive Ordinal Notations}, which are more
concrete and practical (in the long run).

Finally, we will argue that, when applied to AGIs satisfying certain assumptions,
these extended Hibbard intelligence measures
are closely related to the so-called \emph{Intuitive Ordinal Intelligence} measure
which we introduced in \cite{ioi1} and \cite{ioi2}.

The structure of the paper is as follows.
\begin{itemize}
    \item
    In Section \ref{originalmeasuresection}, we review the classical Hibbard measure.
    \item
    In Section \ref{agiperspectivesection}, we present our viewpoint of AGIs and argue that
    under certain idealizing assumptions, \emph{every} AGI should have intelligence
    $\infty$ according to the classical Hibbard measure.
    \item
    In Section ..., we review Intuitive Ordinal Notations.
    \item
    In Section ..., we describe a technique for constructing fast-growing functions
    $f:\mathbb N\to\mathbb N$ from Intuitive Ordinal Notations. This technique is
    closely related to the \emph{majorization hierarchies} from mathematical logic.
    \item
    In Section ..., we introduce a family of large evader-hierarchies and corresponding
    Hibbard intelligence measures.
    \item
    In Section ..., we discuss relationships between Hibbard intelligence measures
    and Intuitive Ordinal Intelligence.
\end{itemize}

\section{Hibbard's original measure}
\label{originalmeasuresection}

Hibbard's intelligence measure is based on predictors and evaders---the predictor
representing the AGI whose intelligence we would like to measure, and the evader
representing an environment---which we define below. A predictor and an evader
are thought of as interacting together in a ``game of adversarial sequence prediction''.

\begin{definition}
By $B$, we mean the binary alphabet $\{0,1\}$. By $B^*$, we mean the set of all
finite binary sequences. By $B^\infty$, we mean the set of all infinite binary
sequences. By $\langle\rangle$ we mean the empty binary sequence.
\end{definition}

\begin{definition}
\label{evaderpredictordefn}
    (Evaders and predictors)
    \begin{enumerate}
        \item
        By an \emph{evader}, we mean a Turing machine $e$
        which takes as input a finite (possibly empty) binary sequence
        $(y_1,\ldots,y_n)\in B^*$
        (thought of as a sequence of \emph{predictions})
        and outputs $0$ or $1$ (thought of as an \emph{evasion}), which output
        we write as $e(y_1,\ldots,y_n)$.
        \item
        By a \emph{predictor}, we mean a function $p:B^*\to B$
        which takes as input a finite (possibly empty) binary sequence
        $(x_1,\ldots,x_n)\in B^*$
        (thought of as a sequence of \emph{evasions})
        and outputs $0$ or $1$ (thought of as a \emph{prediction}).
        \item
        For any evader $e$ and predictor $p$, the \emph{result of $p$ playing the
        game of adversarial prediction against $e$} (or more simply, the \emph{result of
        $p$ playing against $e$}) is the infinite binary sequence
        $(x_1,y_1,x_2,y_2,\ldots)\in B^\infty$
        defined as follows:
        \begin{enumerate}
            \item
            $x_1=e(\langle\rangle)$ is
            the output of $e$ when run on the empty prediction-sequence.
            This is thought of as $e$'s first evasion.
            \item
            $y_1=p(\langle\rangle)$ is
            the result of applying $p$ to the empty evasion-sequence.
            This is thought of as $p$'s first prediction.
            \item
            For all $n>0$,
            $x_{n+1}=e(y_1,\ldots,y_n)$ is
            the output of $e$ on the sequence of $p$'s first $n$ predictions.
            This is thought of as $e$'s $(n+1)$th evasion.
            \item
            For all $n>0$,
            $y_{n+1}=p(x_1,\ldots,x_n)$ is
            the result of applying $p$ to $e$'s first $n$ evasions.
            This is thought of as $p$'s $(n+1)$th prediction.
        \end{enumerate}
        \item
        Suppose $r=(x_1,y_1,x_2,y_2,\ldots)$ is the result of a predictor $p$ playing
        against an evader $e$. For every $n\geq 1$,
        we say that \emph{the predictor wins round $n$ in $r$}
        if $x_n=y_n$; otherwise, if $x_n\neq y_n$, we say that
        \emph{the evader wins round $n$ in $r$}.
        We say that \emph{$p$ learns to predict $e$} if there is some $N\in\mathbb N$
        such that for all $n>N$, $p$ is the winner of round $n$ in $r$.
    \end{enumerate}
\end{definition}

\begin{definition}
    For any evader $e$ and natural $n>0$,
    let
    \[
        t_e(n) = \max_{b_1,\ldots,b_n\in \{0,1\}}
        (\text{number of steps $e$ takes to run on input $(b_1,\ldots,b_n)$}).
    \]
\end{definition}

\begin{example}
Let $e$ be an evader.
\begin{enumerate}
    \item
    $t_e(0)$ is simply the number of steps $e$ takes to run on input $\langle\rangle$,
    the empty binary sequence.
    \item
    $t_e(2)$ is equal to the number of steps $e$ takes to run on input
    $(0,0)$, or to run on input $(0,1)$, or to run on input $(1,0)$, or to run on input
    $(1,1)$---whichever of these four possibilities is largest.
\end{enumerate}
\end{example}

\begin{definition}
    Suppose $f:\mathbb N\to\mathbb N$. We define the \emph{evader-set bounded by $f$},
    written $E_f$, to be the set of all evaders $e$ such that
    there is some $k\in\mathbb N$ such that $\forall n>k$,
    $t_e(n)<f(n)$.
\end{definition}

\begin{definition}
\label{generalintelligencemeasuredefn}
    Suppose $g_i$ ($i\in \mathbb N$) is any family of functions,
    each $g_i:\mathbb N\to\mathbb N$.
    For each $m\in\mathbb N$, define $f_m:\mathbb N\to\mathbb N$ by
    \[f_m(k)=\max_{i\leq m}\max_{j\leq k}g_i(j).\]
    For any predictor $p$, the \emph{Hibbard intelligence of $p$ (given by the family $g_i$)}
    is defined to be the maximum $m$ such that
    $p$ learns to predict $e$ for every $e$ in $E_{f_m}$ (or $\infty$
    if $p$ learns to predict $e$ for every $e$ in $E_{f_m}$ for every $m$).
\end{definition}

\begin{definition}
\label{classichibbardmeasuredefn}
    The \emph{classic Hibbard measure} is defined to be the Hibbard intelligence
    measure given by one specific family $g_i$ of functions, namely:
    the enumeration of the primitive recursive functions described by S.C.\ Liu
    \cite{liu1960enumeration}.
\end{definition}


\section{Do all AGIs have classic Hibbard intelligence $\infty$?}
\label{agiperspectivesection}

As yet, we are unaware of any formal mathematical definition of what an AGI
actually is. We take the stance that an AGI should be capable of understanding
practical conversations and instructions in a common human language (such as
English) and, if commanded by their employers, to obey commands expressed
therein. So if the AGI's employer said, ``Recite digits of pi until I tell you to
stop,'' then the AGI would begin computing and reciting digits of pi, until said
employer told it to stop (if ever). Unlike a human employee, who would eventually
get fed up with such a task, an AGI should have no fear of tedium, and should
never grow tired of performing whatever task it has been assigned.

We think assumptions like the above are implicit in many attempts to study AGIs.
Many authors who set out to study
AGIs actually study not the full AGI, but rather the AGI's persona when it is
performing some general type of task. For example:
\begin{itemize}
    \item
    Legg and Hutter measure the intelligence of AGIs based on their performance
    in reinforcement learning environments \cite{legg}. On a superficial
    level, that approach is inappropriate because a genuine AGI is much more than
    merely a reinforcement learning agent (however good that agent might be).
    But at a deeper level, the approach is appropriate because although an AGI is
    more than an RL agent, we could command the AGI to act as an RL agent, saying,
    ``I am going to put you in a certain environment where you can take such-and-such
    tasks, and rewards and observations will be communicated to you; please try to
    reverse-engineer the environment and act in such a way as to maximize rewards.''
    And while it is difficult to say anything about a full AGI, we can say quite a
    lot about an RL agent, and thus we can study AGIs by proxy.
    \item
    In the Non-Axiomatic Reasoning System research program (NARS) \cite{nars},
    AGIs are reduced to agents who are bombarded with assertions in a specific formal
    language, each assertion being accompanied by a priority measure, and some of
    which assertions might contradict each-other. At any time during the bombardment
    of assertions, the agent might also be queried, at which point the agent's job is
    to estimate the probability of the truth of a queried assertion, based on the
    assertions and priorities it has seen so far. This is certainly not all an AGI
    is (for an AGI should be able to reason about first- and higher-order logics
    involving quantifiers, for example). Nevertheless, an AGI's employer could command
    it to behave like such an agent, and in response the AGI would behave like such an
    agent.
    \item
    Many philosophers and logicians reduce an AGI to a mere \emph{knowing
    agent} who performs no actions but simply knows things in some formal language.
    Superficially, this is inappropriate because an AGI does much more than merely
    know things \cite{wang2007three}. But at a deeper level, this is appropriate
    because an AGI could certainly be commanded to act as such an agent, e.g., via 
    a command like: ``Until further notice, do nothing except enumerate consequences
    of Peano arithmetic'' (or some other formal language). And again, while it is
    hard to say anything about a full AGI, we can say a lot about such computably
    enumerable theories as would be recited by an AGI in response to such a question
    (and they would indeed be computably enumerable because an AGI, whatever else it
    is, must be mechanical).
\end{itemize}

In the same way, an AGI is certainly more than a predictor competing in
games of adversarial sequence prediction. But an AGI could be commanded to
act as such a predictor. That is why it makes sense to refer to the measures
from Definition \ref{generalintelligencemeasuredefn} as ``intelligence'' measures
(rather than as, say, ``prediction power'' measures). In the same way that
IQ tests are intended to measure overall human intelligence despite the fact that
human intelligence is so much more than the ability to take IQ tests, in the same
way, adversarial sequence prediction games can serve as a sort of intelligence
test for AGIs.

In the remainder of this section, we will argue that under certain idealizing
assumptions, every AGI should have classical Hibbard intelligence $\infty$.
Thus, we would suggest that Definition \ref{classichibbardmeasuredefn} is really
only appropriate for certain weak AIs who are strong enough that they can be
induced to participate in adversarial sequence prediction games, but who are
not strong enough to qualify as genuine AGIs. Later in the paper, we will argue
that suitable extensions of Hibbard's intelligence measure are more relevant
for genuine AGIs.

\begin{assumption}
\label{idealizingassumption}
    We make the following idealizing assumptions about an AGI $X$.
    \begin{itemize}
        \item
        (Mathematical truthfulness) If $X$ knows some statement in the language
        of Peano arithmetic, then that statement is true.
        \item
        (Mathematical reasoning ability) If $X$ knows some set $S$ of mathematical
        axioms then, given sufficient time to think about it, $X$ would eventually
        become aware of all the consequences of $S$.
        \item
        (Basic mathematical exposure) $X$ knows the axioms of Peano arithmetic.
        \item
        (Caution) $X$ avoids infinite loops: if $X$ is commanded to play the
        role of a predictor, then the resulting predictor $p$ will have
        the property that $X$ knows that $p$ is a total computable function
        from $B^*$ to $B$.
    \end{itemize}
\end{assumption}

\begin{definition}
\label{Xspredictordefn}
    Suppose $X$ is an AGI.
    \begin{enumerate}
        \item
        \emph{$X$'s predictor} is the predictor which
        would result if $X$ were commanded to play the role of a predictor (so
        the $X$'s predictor is a total computable function from $B^*\to B$ by Caution).
        \item
        For any family $g_i$ of functions as in
        Definition \ref{generalintelligencemeasuredefn},
        the \emph{Hibbard intelligence of $X$ (given by the family $g_i$)}
        is defined to be the Hibbard intelligence of
        $X$'s predictor (given by the family $g_i$).
    \end{enumerate}
\end{definition}

\subsection{A semi-optimal predictor}

\begin{definition}
\label{bruteforcepredictordefn}
    (The Brute-Force Predictor)
    Suppose $X$ is an AGI. Let $M_1,M_2,\ldots$ be the list of Turing machines
    which $X$ would enumerate if $X$ were commanded: ``Enumerate all the Turing
    machines which you know define total functions from $B^*$ to $B$''.
    By \emph{$X$'s brute-force predictor}, we mean the predictor $p$ defined
    as follows.
    \begin{enumerate}
        \item
        Initially, $p$ shall attempt to predict the evader $e$ by assuming that $e$
        defines the same function as $M_1$. When (if ever) this fails,
        say that \emph{$M_1$ is ruled out from being the evader}.
        \item
        Once $M_1$ has been ruled out from being the evader (if ever),
        $p$ shall attempt to predict $e$ by assuming that $e$ defines the same
        function as $M_2$. When (if ever) this fails,
        say that \emph{$M_2$ is ruled out from being the evader}.
        \item
        Once $M_2$ has been ruled out from being the evader (if ever),
        $p$ shall attempt to predict $e$ by assuming that $e$ defines the same
        function as $M_3$. When (if ever) this fails,
        say that \emph{$M_3$ is ruled out from being the evader}.
        \item
        And so on forever...
    \end{enumerate}
\end{definition}

Note that $X$'s predictor (Definition \ref{Xspredictordefn})
may or may not be equal to $X$'s brute-force predictor
(Definition \ref{bruteforcepredictordefn}). Note also that $X$'s brute-force predictor
is total precisely because of the mathematical truthfulness assumption
(from Definition \ref{idealizingassumption}): whenever $X$ knows that
$M_i$ defines a total function from $B^*$ to $B$, $M_i$ really \emph{does}
define a total function from $B^*$ to $B$ (hereafter, we will suppress
remarks like this and routinely use the mathematical truthfulness assumption
without explicit mention).

\begin{lemma}
\label{knowingimplieslearninglemma}
    Let $X$ be an AGI and let $e$ be an evader (so $e$ is a Turing machine,
    say, the $n$th Turing machine) which
    defines a total function from $B^*$ to $B$).
    If $X$ knows that the $n$th Turing machine defines a total function from $B^*$ to $B$,
    then $X$'s brute-force predictor learns to predict $e$.
\end{lemma}

\begin{proof}
    Let $p$ be $X$'s brute-force predictor.
    Let $M_1,M_2,\ldots$ be as in Definition \ref{bruteforcepredictordefn}.
    Since $X$ knows that the $n$th Turing machine (i.e., $e$) defines a total
    function from $B^*$ to $B$,
    it follows that $e=M_k$ for some $k$.
    When $p$ plays against $e$, it cannot occur that $M_k$ is ruled out
    from being the evader, because in order for that to occur, $p$ would have
    to fail at predicting $e$ when $p$ assumes that $e$ computes the same
    function as $M_k$, but that cannot fail because that assumption is true.
    Since $M_k$ cannot be ruled out from being the evader, it follows that
    step $k+1$ of Definition \ref{bruteforcepredictordefn} will never be
    reached, which in turn implies that $p$ stops failing at predicting $e$
    after finitely many initial failures, in other words, $p$ learns to
    predict $e$.
\end{proof}

For any AGI $X$, we would like to claim that $X$'s brute-force predictor is
optimal among all predictors which $X$ could make use of without violating the
Caution assumption (Definition \ref{idealizingassumption}); unfortunately this
is not true in the strongest sense it possibly could be true, as the following
example shows.

\begin{example}
\label{bruteforcenottotallyoptimalexample}
    Let $X$ be an AGI. Let $f:\mathbb N\to B$ be a total computable function which
    is different from every total computable function $X$ knows.
    For each $i\in\{0,1\}$, $e_i$ be an evader defined as follows:
    \[
        e_i(y_1,\ldots,y_n) =
        \begin{cases}
            i &\mbox{if $y_1=\cdots=y_n=i$ (or if $n=0$),}\\
            f(n) &\mbox{otherwise.}
        \end{cases}
    \]
    For each $i\in\{0,1\}$, let $p_i$ be the
    constantly-$i$ predictor, $p_i(x_1,\ldots,x_n)=i$.
    Then $p_0$ learns to predict $e_0$ and $p_1$ learns to predict $e_1$,
    but $X$'s brute-force predictor learns to predict at most one of $e_0$ or $e_1$.
\end{example}

\begin{proof}
    Clearly the result of $p_0$ playing against $e_0$ is
    $(x_0,y_0,x_1,y_1,\ldots)=(0,0,0,0,\ldots)$, so $p_0$ learns to predict $e_0$.
    Likewise, the result of $p_1$ playing against $e_1$ is
    $(x_0,y_0,x_1,y_1,\ldots)=(1,1,1,1,\ldots)$, so $p_1$ learns to predict $e_1$.
    It remains to show that $X$'s brute-force predictor cannot learn to predict
    both $e_0$ and $e_1$.

    Let $p$ be $X$'s brute-force predictor.
    Let $M_1,M_2,\ldots$ be as in Definition \ref{bruteforcepredictordefn}.
    Let $g:B^*\to B$ be the function defined by $M_1$.

    Case 1: $g(\langle\rangle)=1$. Then when $p$ plays against $e_0$,
    after the first step, it will never be the case that
    $y_1=\cdots=y_n=0$ (because $y_1=g(\langle\rangle)=1$).
    Thus, for all $n>0$, $e_0(y_1,\ldots,y_n)=f(n)$.
    I claim that $p$ does not learn $e_0$.
    To see this, assume (for the sake of contradiction)
    that $p$ learns $e_0$. It follows that there is some $k$ (which we may take
    as small as possible) such that $M_k$ never gets ruled out from being the
    evader (Definition \ref{bruteforcepredictordefn}). Let $h:B^*\to B$ be the
    function defined by $M_k$.
    Let $(x_0,y_0,x_1,y_1,\ldots)$ be the result of $p$ playing against $e_0$.
    It follows that there is some $j$ such that for all $i>j$,
    $h(y_1,\ldots,y_{i-1})=x_i=y_i=f(i)$.
    This shows that for all but finitely many $i\in\mathbb N$,
    $f(i)=h(y_1,\ldots,y_{i-1})$. Since $X$ knows that $M_k$ defines a total
    function from $B^*$ to $B$, it follows that $X$ knows $f$ is total computable,
    which contradicts our choice of $f$.

    Case 2: $g(\langle\rangle)=0$. Then by similar reasoning as in Case 1,
    it can be shown that $p$ does not learn $e_1$.
\end{proof}

Example \ref{bruteforcenottotallyoptimalexample} shows that we cannot
hope for the brute-force to be totally optimal in the most extreme possible sense:
there will always be evaders that the AGI's brute-force fails to learn, but which other
predictors (which the AGI knows about)
do nevertheless learn. Example \ref{bruteforcenottotallyoptimalexample} involves
highly contrived evaders which are custom made to act stupidly in one specific case
(allowing them to be learned by a correspondingly stupid predictor), while being
highly sophisticated in all other cases. In the following definition, we rule out
situations where a less sophisticated predictor manages to learn a
more sophisticated evader due to the latter cloaking its true sophistication from
the former.

\begin{definition}
    Suppose $M$ is a Turing machine which computes a predictor $p$.
    Suppose $e$ is an evader.
    We say that \emph{$M$ learns $e$ but-not-without-a-fight}
    if the following conditions hold:
    \begin{enumerate}
        \item $p$ learns $e$.
        \item For each $i$, $t_M(i)\geq t_e(i)$.
    \end{enumerate}
\end{definition}

\begin{theorem}
    (Semi-optimality of brute force)
    Let $X$ be an AGI.
    Suppose $M$ is a Turing machine which computes a predictor $p$.
    If $X$ knows that $M$ computes a predictor, and if $M$ learns $e$
    but-not-without-a-fight, then $X$'s brute-force predictor learns $e$.
\end{theorem}

\begin{proof}
    Since $X$ knows $M$ computes a predictor, $X$ knows $M$ computes a total
    function from $B^*$ to $B$. Thus $X$ knows that that $e'$ is a total
    computable function from $B^*$ to $B$, where $e'$ is a Turing machine which
    takes an input $b\in B^*$ and operates as follows:
    \begin{enumerate}
        \item
        Calculate $t_M(i)$ (by running $M$ on all length-$i$ binary sequences).
        \item
        Run $e$ on $b$ up to $t_M(i)$ steps. If $e$ outputs a result $x$ within that
        time, then output $x$. Otherwise, output $0$.
    \end{enumerate}
    Since $M$ learns $e$ but-not-without-a-fight, each $t_M(i)\geq t_e(i)$,
    so in fact $e'$ computes the same function as $e$.
    By Lemma \ref{knowingimplieslearninglemma}, $X$'s brute-force predictor
    learns $e'$. Since $e'$ and $e$ compute the same function, this implies
    $X$'s brute-force predictor learns $e$.
\end{proof}

\bibliographystyle{plain}
\bibliography{hibbard}
\end{document}
